{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation Protocol\n",
    "\n",
    "| Paper                         | Modalities                        | DL approaches used                     |\n",
    "| -----------                   | -----------                       | -----------                            |\n",
    "| Yang et al. (2021)            | Video (facial + recovered rPPG)   | MTCNN (face crop), Inflated 3D ConvNet |\n",
    "| Rodriguez et al. (2017)       | Video (facial)                    | VGG-16 (VGG_faces) + LSTM              |\n",
    "| Lopez-Martinez et al. (2017)  | Physiological (SC, ECG)           | NN with two FC layers                  |\n",
    "| Zhou et al. (2016)            | Video (facial)                    | RCNN                                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summaries of papers with relevant setups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yang, Ruijing, et al. \"Non-contact Pain Recognition from Video Sequences with Remote Physiological Measurements Prediction.\" arXiv preprint arXiv:2105.08822 (2021).\n",
    "\n",
    "*The objective of the framework is to learn an enriched facial representation that can distinguish different pain intensities. Paper presents rPPG-enriched Spatio-temporal Attention Network (rSTAN) to process a video snippet in two branches: a rPPG recovery branch (Deep-rPPG) to recover rPPG signals as an auxiliary task and a facial representation learning branch (STAN). A Visual Feature Enrichment (VFE) module attempts to complement the facial representation by considering the spatial ROIs from rPPG features with focus on time steps that contain physiological variations related to pain.*\n",
    "\n",
    "*Not a purely multi-modal approach since only video data are used as input.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proposed system**\n",
    "\n",
    "Experimental setup details: Nvidia P100 using PyTorch.\n",
    "\n",
    "1. Original video is downsampled to L = 64\n",
    "\n",
    "2. MTCNN detects and crops the facial area.\n",
    "\n",
    "3. Framework built upon Inflated 3D ConvNet. \n",
    "\n",
    "4. For parameter selection, all subjects are randomly split into 5 folds and 5-fold CV is used to determine the best parameters.\n",
    "\n",
    "5. Adam is used as the optimizer with a LR of 2e-4 which is decayed after 10 epochs with gamma = 0.8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation and Comparison**\n",
    "\n",
    "No similar multi-task framework in literature until now, comparison of results to those with multi-modal fusion schemes. Those multi-modal schemes all use early fusion because rSTAN also performs at feature level, not decision level.\n",
    "\n",
    "Performance also compared to uni-modal (appearance-based) approaches.\n",
    "\n",
    "Comparisons to state-of-the-arts following LOSO protocol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P. Rodriguez et al., \"Deep Pain: Exploiting Long Short-Term Memory Networks for Facial Expression Classification,\" in IEEE Transactions on Cybernetics, doi: 10.1109/TCYB.2017.2662199.\n",
    "\n",
    "*End-to-end (it learns to extract features and also learns to use them to predict the level of pain) DL approach. Approach is based on CNN and apply temporal modeling using LSTM onto the featured learned from the VGG_faces network.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proposed system summary**\n",
    "\n",
    "1. Cropped images using facial landmarks, then frontalized. Used generalized Procrustes analysis (GPA) to align the landmarks. \n",
    "\n",
    "2. Imbalanced dataset (8K pain frames and 40K labeled as no pain). => The training data was balanced by randomly under-sample the majority class (i.e. no-pain) + complemented the results by giving normalized scores (balancing the validation data).\n",
    "\n",
    "3. Target preprocessing: Using MSE as it's very sensitive and most suited for cases where Gaussian noise is present. Labels (the pain levels) are standardized before training. \n",
    "\n",
    "4. Data augmentation: flipping images with 50% probability, adding random noise to the reference landmarks before performing piece-wise affine warping (i.e. introducing small deformations to faces).\n",
    "\n",
    "5. Train a CNN to perform pain level recognition task: fine-tuning VGG-16 CNN pretrained with faces (\"Deep face recognition\" by Parkhi et al.). The CNN_faces model was trained on raw images of faces with some background but in this experiment, the background was black => need to compensate their differences by subtracting the per-pixel mean (NOT global pixel mean). Used L2 between predicted label y_hat and ground truth label y (instead of log-likelihood).\n",
    "\n",
    "6. Using temporal information is achieved by extracting the features from the CNN fc6 layer and train a LSTM. The CNN processes each frame and the outputs of fc6 is a low-dimensional feature vector for each image. M feature vectors have to be grouped together in sequences of length p created so that each frame is the last of a sequence once. Each sequence is labeled with one label corresponding to the label of the last frame of the sequence. As each sequence only has one label, the hidden state of the last time-step is used to compute the output of the network. The LSTM input data needs to be balanced at the sequence level (not frame level) so that no frames are skipped. Frames need to be sorted in time, split them in sequences, and discard entire sequences with no pain in all their frames until they match the number of sequences with pain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation and Comparison**\n",
    "\n",
    "Results are compared against continuous prediction models with the intraclass correlation coefficient (ICC), pearson correlation coefficient (PC), MSE and MAE. Aggregated the pain levels so that 4 and 5 are merged and 6+ becomes the 5th level.\n",
    "\n",
    "A binary threshold is also needed to compare in the case of binary accuracy. Performance is evaluated with skew normalized accuracy and AUC scores (to mitigate the effect of imbalanced test data) on leave-one-subject-out CV since subject-exclusiveness increases the confidence that the model will behave similarly with new data. Accuracies are reported with a threshold of [0,1) for no-pain and [1,∞) for pain. \n",
    "\n",
    "Most of the mistakes in model performance is due to frontier effects. \n",
    "\n",
    "Considering the raw image and temporal information at the pixel level allowed the model to outperform the results obtained by previous canonical normalized appearance approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lopez-Martinez, D.; Picard, R. Multi-task neural networks for personalized pain recognition from physiological signals. InProceedings of the 2017 Seventh International Conference on Affective Computing and Intelligent Interaction Workshops andDemos (ACIIW), San Antonio, TX, USA, 21–27 October 2017; pp. 181–184. \n",
    "\n",
    "*Conducted experiments by testing single-task classifiers (logistic regression and SVM) performances against multi-task neural network. Multi-task learning accounts for individual differences in pain responses while still learning data from across the population.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proposed system summary**\n",
    "\n",
    "Implemented using TensorFlow and Keras. \n",
    "\n",
    "1. Only used SC and ECG signals as input since they can be obtained from wrist-sensors.\n",
    "\n",
    "2.  Multi-task learning involves simultaneously training related tasks over shared representations. It contains the M sigmoid classifiers, one for each task, and optimization of the corresponding loss functions is done simultaneously. The NN consists of an input layer, two FC hidden layers: one person-specific hidden layer with one task defined for each subject in the dataset, and one hidden layer shared between all tasks (hard parameter sharing). \n",
    "\n",
    "3. Upper bound constraint on the norm of the network weights, dropout and early stopping applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation and Comparison**\n",
    "\n",
    "Classification accuracy is estimated via 10-fold CV. Results indicate SC features significantly outperform ECG features, and multi-task achieves best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### J. Zhou, X. Hong, F. Su and G. Zhao, \"Recurrent Convolutional Neural Network Regression for Continuous Pain Intensity Estimation in Video,\" 2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2016, pp. 1535-1543, doi: 10.1109/CVPRW.2016.191.\n",
    "\n",
    "*RCNN predicts pain intensity (PSPI) predictions frame by frame. Average MSE and PCC are calculated for video sequences.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proposed system summary**\n",
    "\n",
    "Theano framework. Experiments carried out on NVIDIA Tesla K80 GPU.\n",
    "\n",
    "1. Each frame from a video sequence is aligned and warped to the same frontal pose.\n",
    "2. Network input is a 3-channel HxW (30x713) = 713x30x3 frame vector sequences. RCNN requires fixed height input => a sliding window is applied. Each frame was flattened into a feature vector (might loose some structural information but preserves temporal information), and all 1D flattened warped facial images are concatenated in frame order to achieve frame vector sequences.\n",
    "3. In each RCL, one convolutional layer is first used and then connected three iterations (T = 3) following the feed-forward layer. In the CL, a linear function is used as the activation to conduct the regression task and MSE function as loss measurement. Output FC is a softmax layer.\n",
    "4. Training is performed by minimizing the MSE function using back-prop through time BPTT algorithm (equivalent to using standard BP algorithm on time-unfolded network). LR = 1/1000 of its initial value. Momentum was fixed at 0.9. Weight decay and dropout. Batch normalization following the first CL and every RCLs to accelerate training.\n",
    "5. The network will output the PSPI predictions frame by frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation and Comparison**\n",
    "\n",
    "LOSO strategy => 25-fold CV (all sequences of one chosen subject was left as the testing set, and the rest sequences as training set). \n",
    "\n",
    "Average MSE and pearson product-moment correlation coefficient (PCC) were calculated for total number of frames y_hat and ground truth y and the pain intensity estimation of the ith frame. \n",
    "\n",
    "Average testing time is 25 frame per second (efficient for real-time?)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
